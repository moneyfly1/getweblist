#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆä»£ç†æœé›†è„šæœ¬
ä¸“é—¨æµ‹è¯•æ–°çš„ä»£ç†æºç½‘ç«™
"""

import sys
import os
import time
import json
from loguru import logger

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from services.utils.proxy_collector import ProxyCollector
from services.utils.proxy_manager import ProxyManager

def test_specific_sources():
    """æµ‹è¯•ç‰¹å®šçš„ä»£ç†æº"""
    collector = ProxyCollector()
    
    # æµ‹è¯•æ–°çš„ä»£ç†æº
    new_sources = [
        '66ip', '89ip', 'Goubanjia', 'PubProxy', 
        'Proxy11', 'ProxyListDownload', 'ProxyListDownloadHTTP', 'ProxyListDownloadSOCKS',
        # æ–°å¢çš„é«˜è´¨é‡ä»£ç†æº
        'SPYS', 'HidemyName', 'FreeProxyCZ', 'SocksProxy', 'Hidester', 
        'Premproxy', 'ProxyScrape', 'OpenProxy', 'ProxyDB',
        'ProxyScrapeAPI', 'ProxyScrapeHTTPS', 'ProxyScrapeSOCKS5'
    ]
    
    print("=" * 60)
    print("æµ‹è¯•æ–°çš„ä»£ç†æº")
    print("=" * 60)
    
    for source_name in new_sources:
        print(f"\nğŸ” æµ‹è¯•ä»£ç†æº: {source_name}")
        
        # æ‰¾åˆ°å¯¹åº”çš„æºé…ç½®
        source_config = None
        for source in collector.proxy_sources:
            if source['name'] == source_name:
                source_config = source
                break
        
        if source_config:
            try:
                proxies = collector.fetch_proxies_from_source(source_config)
                if proxies:
                    print(f"âœ… {source_name}: è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
                    print(f"   å‰5ä¸ªä»£ç†: {proxies[:5]}")
                else:
                    print(f"âŒ {source_name}: æ²¡æœ‰è·å–åˆ°ä»£ç†")
            except Exception as e:
                print(f"âŒ {source_name}: è·å–å¤±è´¥ - {e}")
        else:
            print(f"âŒ {source_name}: æœªæ‰¾åˆ°é…ç½®")

def test_all_sources():
    """æµ‹è¯•æ‰€æœ‰ä»£ç†æº"""
    collector = ProxyCollector()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ‰€æœ‰ä»£ç†æº")
    print("=" * 60)
    
    all_proxies = []
    
    for source in collector.proxy_sources:
        print(f"\nğŸ” æµ‹è¯•: {source['name']}")
        try:
            proxies = collector.fetch_proxies_from_source(source)
            if proxies:
                print(f"âœ… è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
                all_proxies.extend(proxies)
            else:
                print(f"âŒ æ²¡æœ‰è·å–åˆ°ä»£ç†")
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥: {e}")
    
    # å»é‡
    unique_proxies = list(set(all_proxies))
    print(f"\nğŸ“Š æ€»è®¡è·å–åˆ° {len(unique_proxies)} ä¸ªå”¯ä¸€ä»£ç†")
    
    return unique_proxies

def test_proxies_quality(proxies):
    """æµ‹è¯•ä»£ç†è´¨é‡"""
    if not proxies:
        print("âŒ æ²¡æœ‰ä»£ç†å¯æµ‹è¯•")
        return []
    
    print(f"\nğŸ§ª æµ‹è¯• {len(proxies)} ä¸ªä»£ç†çš„è´¨é‡...")
    
    # åªæµ‹è¯•å‰50ä¸ªä»£ç†ä»¥èŠ‚çœæ—¶é—´
    test_proxies = proxies[:50]
    working_proxies = []
    
    for i, proxy in enumerate(test_proxies, 1):
        print(f"æµ‹è¯• {i}/{len(test_proxies)}: {proxy}")
        
        try:
            result = collector.test_proxy(proxy)
            if result:
                working_proxies.append(result)
                print(f"âœ… {proxy} å¯ç”¨ (é€Ÿåº¦: {result['speed']:.2f}s)")
            else:
                print(f"âŒ {proxy} ä¸å¯ç”¨")
        except Exception as e:
            print(f"âŒ {proxy} æµ‹è¯•å¤±è´¥: {e}")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
        time.sleep(0.5)
    
    return working_proxies

def save_proxy_report(proxies, working_proxies):
    """ä¿å­˜ä»£ç†æŠ¥å‘Š"""
    report = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_proxies': len(proxies),
        'working_proxies': len(working_proxies),
        'success_rate': f"{(len(working_proxies) / len(proxies) * 100):.2f}%" if proxies else "0%",
        'working_proxies_list': working_proxies
    }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open('proxy_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜å¯ç”¨ä»£ç†åˆ°æ–‡æœ¬æ–‡ä»¶
    with open('working_proxies.txt', 'w', encoding='utf-8') as f:
        for proxy_info in working_proxies:
            f.write(f"{proxy_info['proxy']}\n")
    
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"   - proxy_report.json: è¯¦ç»†æŠ¥å‘Š")
    print(f"   - working_proxies.txt: å¯ç”¨ä»£ç†åˆ—è¡¨")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å¢å¼ºç‰ˆä»£ç†æœé›†å·¥å…·")
    print("=" * 60)
    
    # æµ‹è¯•ç‰¹å®šæº
    test_specific_sources()
    
    # æµ‹è¯•æ‰€æœ‰æº
    all_proxies = test_all_sources()
    
    if all_proxies:
        # æµ‹è¯•ä»£ç†è´¨é‡
        working_proxies = test_proxies_quality(all_proxies)
        
        # ä¿å­˜æŠ¥å‘Š
        save_proxy_report(all_proxies, working_proxies)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ‰ æœé›†å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»ä»£ç†æ•°: {len(all_proxies)}")
        print(f"   - å¯ç”¨ä»£ç†æ•°: {len(working_proxies)}")
        if all_proxies:
            success_rate = len(working_proxies) / len(all_proxies) * 100
            print(f"   - æˆåŠŸç‡: {success_rate:.2f}%")
        
        if working_proxies:
            print(f"\nğŸ† æœ€ä½³ä»£ç† (æŒ‰é€Ÿåº¦æ’åº):")
            sorted_proxies = sorted(working_proxies, key=lambda x: x['speed'])
            for i, proxy_info in enumerate(sorted_proxies[:10], 1):
                print(f"   {i}. {proxy_info['proxy']} (é€Ÿåº¦: {proxy_info['speed']:.2f}s)")
        
        print(f"\nğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œé‡‡é›†å™¨:")
        print(f"python src/main.py mining --collector --classifier")
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•ä»£ç†")

if __name__ == "__main__":
    main() 